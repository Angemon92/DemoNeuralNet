{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Deep Learning from Scratch\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "A linear regression assumes that the true function (we are trying to replicate) can be written as:\n",
    "\n",
    "\\begin{equation*} \n",
    "   y = X\\beta + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "In other words the true function is a linear combination of its parameters.\n",
    "\n",
    "The vector of residuals given an estimate of $\\beta$ is thus:\n",
    "\n",
    "\\begin{equation*} \n",
    "   e = y - X\\beta^{OLS}\n",
    "\\end{equation*}\n",
    "\n",
    "Where OLS indicates that this is the Ordinary Least Squares estimate of $\\beta$\n",
    "\n",
    "The OLS by definition estimate minimises the sum of squared residuals:\n",
    "\n",
    "\\begin{equation*} \n",
    "   e'e = (y - X\\beta^{OLS})'(y - X\\beta^{OLS})\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "   e'e = y'y - y'X\\beta^{OLS} - \\beta^{OLS'}X'y + \\beta^{OLS'}X'X\\beta^{OLS'}\n",
    "\\end{equation*}\n",
    "\n",
    "Since the tranpose of a scalar is a scalar: $y'X\\beta^{OLS} = (y'X\\beta^{OLS})' = \\beta^{OLS'}X'y$ we get the Residual Sum of Squares (RSS):\n",
    "\n",
    "\\begin{equation*} \n",
    "   RSS = e'e = y'y - 2\\beta^{OLS'}X'y + \\beta^{OLS'}X'X\\beta^{OLS'}\n",
    "\\end{equation*}\n",
    "  \n",
    "We take the derivative of this w.r.t to beta-hat and set it equal to 0:\n",
    "\n",
    "\\begin{equation*} \n",
    "   0 = -2X'y + 2X'X\\beta^{OLS}\n",
    "\\end{equation*}\n",
    "\n",
    "The chart below shows an example of such a function; for the example we assume that wages are a linear function of height:\n",
    "\n",
    "```R\n",
    "X <- runif(100, -5, 5)  # Height\n",
    "y <- X + rnorm(100) + 3  # Wages\n",
    "```\n",
    "\n",
    "The line-of-best-fit here is a line that minimises the squared sum of residuals, it has a slope of 0.95 and an intercept of 2.95.\n",
    "\n",
    "![Figure1](pic_support/linearreg_0.png)\n",
    "\n",
    "Assuming that $X'X$ is a positive definite matrix (our variables are not a perfect linear combination of each other & we have more observations than variables) we can find a closed-form solution for $\\beta^{OLS}$:\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\beta^{OLS} = (X'X)^{-1}X'y\n",
    "\\end{equation*}\n",
    "\n",
    "In R we can run:\n",
    "\n",
    "```R\n",
    "X_mat <- as.matrix(X)\n",
    "# Add column of 1s for intercept coefficient\n",
    "intcpt <- rep(1, length(y))\n",
    "# Combine predictors with intercept\n",
    "X_mat <- cbind(intcpt, X_mat)\n",
    "# OLS (closed-form solution)\n",
    "beta_hat <- solve(t(X_mat) %*% X_mat) %*% t(X_mat) %*% y\n",
    "```\n",
    "\n",
    "To obtain: 2.95, 0.95\n",
    "\n",
    "However, we can also use an interative method known as **Gradient Descent**, this is a generic method for continuous optimisation. With GD we randomly initialise $\\beta^{GD}$ and then calculate the residual (error) and move in the opposite direction to the gradient by a small amount proportional to a parameter we call the **learning-rate**. GD is a bit like rolling a ball down a hill - it will gradually converge to a stationary-point. If the function is convex with a small enough step-size (learning-rate) and high-enough number of iterations we are guaranteed to find a global minimiser. **Stochastic Gradient Descent** is usually used for neural-networks to avoid getting stuck in a local minimum due to a non-convex cost function (along with other methods).\n",
    "\n",
    "The general-formula for GD:\n",
    "\n",
    "1. Find a cost-function\n",
    "2. Randomly initialise your $\\beta$ vector\n",
    "3. Get the derivative of the cost-function given $\\beta$\n",
    "4. Move the $\\beta$ vector in the opposite direction to the gradient\n",
    "\n",
    "In the case of this linear-regression:\n",
    "\n",
    "Our cost-function is the RSS: $y'y - 2\\beta^{OLS'}X'y + \\beta^{OLS'}X'X\\beta^{OLS'}$\n",
    "\n",
    "The derivative of the cost-function is: $-2X'y + 2X'X\\beta^{OLS}$\n",
    "\n",
    "This can be simplified to: $2X'(X\\beta^{OLS} - y)$\n",
    "\n",
    "So, we can write our 'delta' as:\n",
    "\n",
    "\\begin{equation*} \n",
    "    \\frac{dLoss}{d\\beta} = \\frac{2}{N}\\sum_ix_i(x_i\\beta^{OLS} - y)\n",
    "\\end{equation*}\n",
    "\n",
    "This means our equation for $\\beta^{OLS}$ becomes:\n",
    "\n",
    "\\begin{equation*} \n",
    "    \\beta^{OLS} = \\beta^{OLS} - \\frac{lr}{N}\\sum_ix_i(x_i\\beta^{OLS} - y)\n",
    "\\end{equation*}\n",
    "\n",
    "```R\n",
    "for (j in 1:epochs)\n",
    "{\n",
    "    residual <- (X_mat %*% beta_hat) - y\n",
    "    delta <- (t(X_mat) %*% residual) * (1/nrow(X_mat))\n",
    "    beta_hat <- beta_hat - (lr*delta)\n",
    "}\n",
    "```\n",
    "\n",
    "With learning-rate set to 0.1 and epochs set to 200 we converge to the same result: 2.95, 0.95. We can track how the line-of-best has been gradually fitted with this method by plotting it at each iteration:\n",
    "\n",
    "![Figure1](pic_support/linearreg_2.png)\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "A logistic regression is a linear regression that outputs a number bounded between 0 and 1. This means it is useful for classification problems, where we want to predict the probability of something happening. A binomial logistic regression is used when there are just two-classes, to extend beyound two-classes we would typically use a multi-nomial logistic regression.\n",
    "\n",
    "Consider the iris-dataset where we try to predict whether a flower is \"virginica\" or \"versicolor\" by only looking at petal-length and sepal-length. We fit a linear line to 'best' split the categories:\n",
    "\n",
    "![Figure1](pic_support/logit_0.png)\n",
    "\n",
    "The above line has an intercept of -39.84, and coefficient of -31.73 for sepal-length and 105.17 for petal-length. These estimates are obtained by maximising the likelihood.\n",
    "\n",
    "Because the log function is monotone, maximizing the likelihood is the same as maximizing the log-likelihood (or minimising the negative of the log-likelihood)\n",
    "\n",
    "\\begin{equation*} \n",
    "   l_x(\\theta) = \\log L_x(\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "For many reasons it is more convenient to use log likelihood rather than likelihood:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\log L_x\n",
    "   =\n",
    "   \\sum_{i=1}^{N} y_i\\beta^Tx_i - \\log(1+e^{\\beta^Tx_i})  \n",
    "\\end{equation*}\n",
    "\n",
    "```R\n",
    "log_likelihood <- function(X_mat, y, beta_hat)\n",
    "{\n",
    "  scores <- X_mat %*% beta_hat\n",
    "  ll <- (y * scores) - log(1+exp(scores))\n",
    "  sum(ll)\n",
    "}\n",
    "```\n",
    "\n",
    "The log-likelihood in this example is -11.92.\n",
    "\n",
    "Typically [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) or other numerical optimisation procedures are used to minimise the cost/max log-likelihood instead of GD, because the parameter space is pretty smaller (compared to neural-networks).\n",
    "\n",
    "It can be shown that the derivative of the log-likelihood is:\n",
    "\n",
    "\\begin{equation*}\n",
    "     delta = X'(y - prediction)\n",
    "\\end{equation*}\n",
    "\n",
    "Where $prediction = \\sigma(X\\beta)$ and the sigma function (the activation/link function) that transforms our score into a probability is given by: $\\sigma(z)=\\frac{1}{1+e^-z}$\n",
    "\n",
    "The process for using GD for a logistic regression is similar to that of a simple linear-regression:\n",
    "\n",
    "```R\n",
    "# Calculate activation function (sigmoid for logit)\n",
    "sigmoid <- function(z){1.0/(1.0+exp(-z))}\n",
    "\n",
    "logistic_reg <- function(X, y, epochs, lr)\n",
    "{\n",
    "  X_mat <- cbind(1, X)\n",
    "  beta_hat <- matrix(1, nrow=ncol(X_mat))\n",
    "  for (j in 1:epochs)\n",
    "  {\n",
    "    residual <- sigmoid(X_mat %*% beta_hat) - y\n",
    "    # Update weights with gradient descent\n",
    "    delta <- t(X_mat) %*% as.matrix(residual, ncol=nrow(X_mat)) *  (1/nrow(X_mat))\n",
    "    beta_hat <- beta_hat - (lr*delta)\n",
    "  }\n",
    "  # Print log-likliehood\n",
    "  print(log_likelihood(X_mat, y, beta_hat))\n",
    "  # Return\n",
    "  beta_hat\n",
    "}\n",
    "```\n",
    "\n",
    "The only major difference is that we apply a sigmoid function to our prediction - to turn it into a probability. Below we can see why: the output is bounded between 0 and 1:\n",
    "\n",
    "![Figure1](pic_support/logit_1.png)\n",
    "\n",
    "The shape of the sigmoid curve also means that we can increase the speed of convergence by scaling the variables to be closer to 0 - where the gradient is high. Imagine our inputs have a value of 100 - this can create a very high error, however the gradient is nearly flat and thus the update to the coefficients will be tiny.\n",
    "\n",
    "We run the below to optimise our logistic regression using GD:\n",
    "\n",
    "```R\n",
    "beta_hat <- logistic_reg(X, y, 300000, 5)\n",
    "```\n",
    "\n",
    "We match the original results with the coefficients: -38.84, -31.73, 105.17\n",
    "\n",
    "![Figure1](pic_support/logit_2.png)\n",
    "\n",
    "### Neural Network \n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
