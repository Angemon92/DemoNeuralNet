{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Deep Learning from Scratch\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "A linear regression assumes that the true function (we are trying to replicate) can be written as:\n",
    "\n",
    "\\begin{equation*} \n",
    "   y = X\\beta + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "The vector of errors/residuals is given by:\n",
    "\n",
    "\\begin{equation*} \n",
    "   e = y - X\\beta^{OLS}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\beta^{OLS}$ is our estimator for $\\beta$. The chart below shows an example of such a function. The line-of-best-fit here is a line that minimises the sum of residuals, it has a slope of 0.95 and an intercept of 2.95.\n",
    "\n",
    "![Figure1](pic_support/linearreg_0.png)\n",
    "\n",
    "Assuming that $X'X$ is a positive definite matrix (our variables are not a perfect linear combination of each other & we have more observations than variables) we can find a closed-form solution for $X\\beta^{OLS}$:\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\beta^{OLS} = (X'X)^{-1}X'y\n",
    "\\end{equation*}\n",
    "\n",
    "In R we can run:\n",
    "\n",
    "```R\n",
    "X_mat <- as.matrix(X)\n",
    "# Add column of 1s for intercept coefficient\n",
    "intcpt <- rep(1, length(y))\n",
    "# Combine predictors with intercept\n",
    "X_mat <- cbind(intcpt, X_mat)\n",
    "# OLS (closed-form solution)\n",
    "beta_hat <- solve(t(X_mat) %*% X_mat) %*% t(X_mat) %*% y\n",
    "```\n",
    "\n",
    "To obtain: 2.95, 0.95\n",
    "\n",
    "However, we can also use an interative method known as **Gradient Descent**, this is a generic method for continuous optimisation. With GD we randomly initialise $\\beta^{GD}$ and then calculate the residual (error) and move in the opposite direction to the gradient by a small amount proportional to a parameter we call the **learning-rate**. GD is a bit like rolling a ball down a hill - it will gradually converge to a stationary-point. If the function is convex with a small enough step-size (learning-rate) and high-enough number of iterations we are guaranteed to find a global minimiser. **Stochastic Gradient Descent** is usually used for neural-networks to avoid getting stuck in a local minimum due to a non-convex cost function (along with other methods).\n",
    "\n",
    "```R\n",
    "for (j in 1:epochs)\n",
    "{\n",
    "    residual <- (X_mat %*% beta_hat) - y\n",
    "    delta <- (t(X_mat) %*% residual) * (1/nrow(X_mat))\n",
    "    beta_hat <- beta_hat - (lr*delta)\n",
    "}\n",
    "```\n",
    "\n",
    "With learning-rate set to 0.1 and epochs set to 200 we converge to the same result: 2.95, 0.95. We can track how the line-of-best has been gradually fitted with this method by plotting it at each iteration:\n",
    "\n",
    "![Figure1](pic_support/linearreg_2.png)\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "A logistic regression is a linear regression that outputs a number bounded between 0 and 1. This means it is useful for classification problems, where we want to predict the probability of something happening. A binomial logistic regression is used when there are just two-classes, to extend beyound two-classes we would typically use a multi-nomial logistic regression.\n",
    "\n",
    "Consider the iris-dataset where we try to predict whether a flower is \"virginica\" or \"versicolor\" by only looking at petal-length and sepal-length. We fit a linear line to 'best' split the categories:\n",
    "\n",
    "![Figure1](pic_support/logit_0.png)\n",
    "\n",
    "The above line has an intercept of -39.84, and coefficient of -31.73 for sepal-length and 105.17 for petal-length. These estimates are obtained by maximising the likelihood.\n",
    "\n",
    "Because the log function is monotone, maximizing the likelihood is the same as maximizing the log likelihood\n",
    "\n",
    "\\begin{equation*} \n",
    "   l_x(\\theta) = \\log L_x(\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "For many reasons it is more convenient to use log likelihood rather than likelihood:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\log L_x\n",
    "   =\n",
    "   \\sum_{i=1}^{N} y_i\\beta^Tx_i - \\log(1+e^{\\beta^Tx_i})  \n",
    "\\end{equation*}\n",
    "\n",
    "The log-likelihood in this example is -11.92.\n",
    "\n",
    "Typically [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) or other numerical optimisation procedures are used to minimise the cost/max log-likelihood instead of GD, because the parameter space is pretty smaller (compared to neural-networks).\n",
    "\n",
    "The process for using GD for a logistic regression is similar to that of a simple linear-regression:\n",
    "\n",
    "```R\n",
    "# Calculate activation function (sigmoid for logit)\n",
    "sigmoid <- function(z){1.0/(1.0+exp(-z))}\n",
    "\n",
    "logistic_reg <- function(X, y, epochs, lr)\n",
    "{\n",
    "  X_mat <- cbind(1, X)\n",
    "  beta_hat <- matrix(1, nrow=ncol(X_mat))\n",
    "  for (j in 1:epochs)\n",
    "  {\n",
    "    residual <- sigmoid(X_mat %*% beta_hat) - y\n",
    "    # Update weights with gradient descent\n",
    "    delta <- t(X_mat) %*% as.matrix(residual, ncol=nrow(X_mat)) *  (1/nrow(X_mat))\n",
    "    beta_hat <- beta_hat - (lr*delta)\n",
    "  }\n",
    "  # Print log-likliehood\n",
    "  print(log_likelihood(X_mat, y, beta_hat))\n",
    "  # Return\n",
    "  beta_hat\n",
    "}\n",
    "```\n",
    "\n",
    "The only major difference is that we apply a sigmoid function to our prediction - to turn it into a probability. Below we can see why: the output is bounded between 0 and 1:\n",
    "\n",
    "![Figure1](pic_support/logit_1.png)\n",
    "\n",
    "The shape of the sigmoid curve also means that we can increase the speed of convergence by scaling the variables to be closer to 0 - where the gradient is high. Imagine our inputs have a value of 100 - this can create a very high error, however the gradient is nearly flat and thus the update to the coefficients will be tiny.\n",
    "\n",
    "We run the below to optimise our logistic regression using GD:\n",
    "\n",
    "```R\n",
    "beta_hat <- logistic_reg(X, y, 300000, 5)\n",
    "```\n",
    "\n",
    "We match the original results with the coefficients: -38.84, -31.73, 105.17\n",
    "\n",
    "![Figure1](pic_support/logit_2.png)\n",
    "\n",
    "### Neural Network \n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
