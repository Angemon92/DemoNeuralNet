{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Deep Learning from Scratch\n",
    "\n",
    "### [Linear Regression](01_LinearRegression.ipynb)\n",
    "\n",
    "A linear regression assumes that the true function (we are trying to replicate) can be written as:\n",
    "\n",
    "\\begin{equation*} \n",
    "   y = X\\beta + \\epsilon\n",
    "\\end{equation*}\n",
    "\n",
    "In other words the true function is a linear combination of its parameters.\n",
    "\n",
    "The vector of residuals given an estimate of $\\beta$ is thus:\n",
    "\n",
    "\\begin{equation*} \n",
    "   e = y - X\\beta^{OLS}\n",
    "\\end{equation*}\n",
    "\n",
    "Where OLS indicates that this is the Ordinary Least Squares estimate of $\\beta$\n",
    "\n",
    "The OLS by definition estimate minimises the sum of squared residuals:\n",
    "\n",
    "\\begin{equation*} \n",
    "   e'e = (y - X\\beta^{OLS})'(y - X\\beta^{OLS})\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "   e'e = y'y - y'X\\beta^{OLS} - \\beta^{OLS'}X'y + \\beta^{OLS'}X'X\\beta^{OLS'}\n",
    "\\end{equation*}\n",
    "\n",
    "Since the tranpose of a scalar is a scalar: $y'X\\beta^{OLS} = (y'X\\beta^{OLS})' = \\beta^{OLS'}X'y$ we get the Residual Sum of Squares (RSS):\n",
    "\n",
    "\\begin{equation*} \n",
    "   RSS = e'e = y'y - 2\\beta^{OLS'}X'y + \\beta^{OLS'}X'X\\beta^{OLS'}\n",
    "\\end{equation*}\n",
    "  \n",
    "We take the derivative of this w.r.t to beta-hat and set it equal to 0:\n",
    "\n",
    "\\begin{equation*} \n",
    "   0 = -2X'y + 2X'X\\beta^{OLS}\n",
    "\\end{equation*}\n",
    "\n",
    "The chart below shows an example of such a function; for the example we assume that wages are a linear function of height:\n",
    "\n",
    "```R\n",
    "X <- runif(100, -5, 5)  # Height\n",
    "y <- X + rnorm(100) + 3  # Wages\n",
    "```\n",
    "\n",
    "The line-of-best-fit here is a line that minimises the squared sum of residuals, it has a slope of 0.95 and an intercept of 2.95.\n",
    "\n",
    "![Figure1](pic_support/linearreg_0.png)\n",
    "\n",
    "Assuming that $X'X$ is a positive definite matrix (our variables are not a perfect linear combination of each other & we have more observations than variables) we can find a closed-form solution for $\\beta^{OLS}$:\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\beta^{OLS} = (X'X)^{-1}X'y\n",
    "\\end{equation*}\n",
    "\n",
    "In R we can run:\n",
    "\n",
    "```R\n",
    "X_mat <- as.matrix(X)\n",
    "# Add column of 1s for intercept coefficient\n",
    "intcpt <- rep(1, length(y))\n",
    "# Combine predictors with intercept\n",
    "X_mat <- cbind(intcpt, X_mat)\n",
    "# OLS (closed-form solution)\n",
    "beta_hat <- solve(t(X_mat) %*% X_mat) %*% t(X_mat) %*% y\n",
    "```\n",
    "\n",
    "To obtain: 2.95, 0.95\n",
    "\n",
    "However, we can also use an interative method known as **Gradient Descent**, this is a generic method for continuous optimisation. With GD we randomly initialise $\\beta^{GD}$ and then calculate the residual (error) and move in the opposite direction to the gradient by a small amount proportional to a parameter we call the **learning-rate**. GD is a bit like rolling a ball down a hill - it will gradually converge to a stationary-point. If the function is convex with a small enough step-size (learning-rate) and high-enough number of iterations we are guaranteed to find a global minimiser. **Stochastic Gradient Descent** is usually used for neural-networks to avoid getting stuck in a local minimum due to a non-convex cost function (along with other methods).\n",
    "\n",
    "The general-formula for GD:\n",
    "\n",
    "1. Find a cost-function\n",
    "2. Randomly initialise your $\\beta$ vector\n",
    "3. Get the derivative of the cost-function given $\\beta$\n",
    "4. Move the $\\beta$ vector in the opposite direction to the gradient\n",
    "\n",
    "In the case of this linear-regression:\n",
    "\n",
    "Our cost-function is the Mean Squared Error (MSE) which is:\n",
    "\n",
    "$MSE = \\frac{RSS}{N}$\n",
    "\n",
    "and:\n",
    "\n",
    "$RSS: y'y - 2\\beta^{OLS'}X'y + \\beta^{OLS'}X'X\\beta^{OLS'}$\n",
    "\n",
    "The derivative of the RSS is: $-2X'y + 2X'X\\beta^{OLS}$\n",
    "\n",
    "This can be simplified to: $2X'(X\\beta^{OLS} - y)$\n",
    "\n",
    "So, we can write our 'delta' as:\n",
    "\n",
    "\\begin{equation*} \n",
    "    \\frac{dLoss}{d\\beta} = \\frac{2}{N}\\sum_ix_i(x_i\\beta^{OLS} - y)\n",
    "\\end{equation*}\n",
    "\n",
    "This means our equation for $\\beta^{OLS}$ becomes:\n",
    "\n",
    "\\begin{equation*} \n",
    "    \\beta^{OLS} = \\beta^{OLS} - \\frac{lr}{N}\\sum_ix_i(x_i\\beta^{OLS} - y)\n",
    "\\end{equation*}\n",
    "\n",
    "```R\n",
    "for (j in 1:epochs)\n",
    "{\n",
    "    residual <- (X_mat %*% beta_hat) - y\n",
    "    delta <- (t(X_mat) %*% residual) * (1/nrow(X_mat))\n",
    "    beta_hat <- beta_hat - (lr*delta)\n",
    "}\n",
    "```\n",
    "\n",
    "With learning-rate set to 0.1 and epochs set to 200 we converge to the same result: 2.95, 0.95. We can track how the line-of-best has been gradually fitted with this method by plotting it at each iteration:\n",
    "\n",
    "![Figure1](pic_support/linearreg_2.png)\n",
    "\n",
    "### [Logistic Regression](02_LogisticRegression.ipynb)\n",
    "\n",
    "A logistic regression is a linear regression that outputs a number bounded between 0 and 1. This means it is useful for classification problems, where we want to predict the probability of something happening. A binomial logistic regression is used when there are just two-classes, to extend beyound two-classes we would typically use a multi-nomial logistic regression.\n",
    "\n",
    "Consider the iris-dataset where we try to predict whether a flower is \"virginica\" or \"versicolor\" by only looking at petal-length and sepal-length. We fit a linear line to 'best' split the categories:\n",
    "\n",
    "![Figure1](pic_support/logit_0.png)\n",
    "\n",
    "The above line has an intercept of -39.84, and coefficient of -31.73 for sepal-length and 105.17 for petal-length. These estimates are obtained by maximising the likelihood.\n",
    "\n",
    "Because the log function is monotone, maximizing the likelihood is the same as maximizing the log-likelihood (or minimising the negative of the log-likelihood)\n",
    "\n",
    "\\begin{equation*} \n",
    "   l_x(\\theta) = \\log L_x(\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "For many reasons it is more convenient to use log likelihood rather than likelihood:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\log L_x\n",
    "   =\n",
    "   \\sum_{i=1}^{N} y_i\\beta^Tx_i - \\log(1+e^{\\beta^Tx_i})  \n",
    "\\end{equation*}\n",
    "\n",
    "```R\n",
    "log_likelihood <- function(X_mat, y, beta_hat)\n",
    "{\n",
    "  scores <- X_mat %*% beta_hat\n",
    "  ll <- (y * scores) - log(1+exp(scores))\n",
    "  sum(ll)\n",
    "}\n",
    "```\n",
    "\n",
    "The log-likelihood in this example is -11.92.\n",
    "\n",
    "Typically [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) or other numerical optimisation procedures are used to minimise the cost/max log-likelihood instead of GD, because the parameter space is pretty smaller (compared to neural-networks).\n",
    "\n",
    "It can be shown that the derivative of the log-likelihood is:\n",
    "\n",
    "\\begin{equation*}\n",
    "     delta = X'(y - prediction)\n",
    "\\end{equation*}\n",
    "\n",
    "Where $prediction = \\sigma(X\\beta)$ and the sigma function (the activation/link function) that transforms our score into a probability is given by: $\\sigma(z)=\\frac{1}{1+e^-z}$\n",
    "\n",
    "The process for using GD for a logistic regression is similar to that of a simple linear-regression:\n",
    "\n",
    "```R\n",
    "# Calculate activation function (sigmoid for logit)\n",
    "sigmoid <- function(z){1.0/(1.0+exp(-z))}\n",
    "\n",
    "logistic_reg <- function(X, y, epochs, lr)\n",
    "{\n",
    "  X_mat <- cbind(1, X)\n",
    "  beta_hat <- matrix(1, nrow=ncol(X_mat))\n",
    "  for (j in 1:epochs)\n",
    "  {\n",
    "    residual <- sigmoid(X_mat %*% beta_hat) - y\n",
    "    # Update weights with gradient descent\n",
    "    delta <- t(X_mat) %*% as.matrix(residual, ncol=nrow(X_mat)) *  (1/nrow(X_mat))\n",
    "    beta_hat <- beta_hat - (lr*delta)\n",
    "  }\n",
    "  # Print log-likliehood\n",
    "  print(log_likelihood(X_mat, y, beta_hat))\n",
    "  # Return\n",
    "  beta_hat\n",
    "}\n",
    "```\n",
    "\n",
    "The only major difference is that we apply a sigmoid function to our prediction - to turn it into a probability. Below we can see why: the output is bounded between 0 and 1:\n",
    "\n",
    "![Figure1](pic_support/logit_1.png)\n",
    "\n",
    "The shape of the sigmoid curve also means that we can increase the speed of convergence by scaling the variables to be closer to 0 - where the gradient is high. Imagine our inputs have a value of 100 - this can create a very high error, however the gradient is nearly flat and thus the update to the coefficients will be tiny.\n",
    "\n",
    "We run the below to optimise our logistic regression using GD:\n",
    "\n",
    "```R\n",
    "beta_hat <- logistic_reg(X, y, 300000, 5)\n",
    "```\n",
    "\n",
    "We match the original results with the coefficients: -38.84, -31.73, 105.17\n",
    "\n",
    "![Figure1](pic_support/logit_2.png)\n",
    "\n",
    "### [Neural Network](03_NeuralNet.ipynb)\n",
    "\n",
    "In the previous scenarios we used the mean-squared-error to represent our cost-function:\n",
    "\\begin{equation*} \n",
    "   C = \\frac{1}{2n}\\sum_x\\|(y(x) - a(x)\\|^2\n",
    "\\end{equation*}\n",
    "\n",
    "For classification problems with neural-networks we will now use the cross-entropy cost:\n",
    "\\begin{equation*} \n",
    "   C = -\\frac{1}{n}\\sum_xy(x)\\ln(a(x)) + (1 - y(x))\\ln(1-a(x))\n",
    "\\end{equation*}\n",
    "\n",
    "Where $a=\\sigma(\\sum_iw_ix_i + b) = \\sigma(z)$\n",
    "\n",
    "We can show that:\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\frac{dC}{dw_i} = \\frac{1}{N}\\sum_jx_j(\\sigma(z)-y)\n",
    "\\end{equation*}\n",
    "\n",
    "```R\n",
    "cost_delta <- function(method, z, a, y) {if (method=='ce'){return (a-y)}}\n",
    "```\n",
    "\n",
    "This means that the bigger the error, the faster our weight will learn.\n",
    "\n",
    "Our main neural-network functions are:\n",
    "\n",
    "```R\n",
    "neuralnetwork <- function(sizes, training_data, epochs, mini_batch_size, lr, C,\n",
    "                          verbose=FALSE, validation_data=training_data)\n",
    "                          \n",
    "SGD <- function(training_data, epochs, mini_batch_size, lr, C, sizes, num_layers, biases, weights,\n",
    "                verbose=FALSE, validation_data)    \n",
    "                \n",
    "update_mini_batch <- function(mini_batch, lr, C, sizes, num_layers, biases, weights)\n",
    "\n",
    "backprop <- function(x, y, C, sizes, num_layers, biases, weights)\n",
    "```\n",
    "\n",
    "The ```neuralnetwork``` function's main job is to initialise the weight and bias matricies given a list of sizes. For example, if we have 10 variables to predict 4 possible classes and we want a hidden-layer with 20 neurons we would pass: ```c(10,20,4)``` to this function. It passes these matricies to the ```SGD``` function to commence training.\n",
    "\n",
    "The ```SGD``` function splits the training-data into random mini-batches and sends them off to the ```update_mini_batch function```, which calculates the deltas for a batch (using `backprop`) and then updates the weights and bias matricies - so these are held constant **within** a batch:\n",
    "\n",
    "```R\n",
    "# After mini-batch has finished update biases and weights: \n",
    "# Opposite direction of gradient\n",
    "weights <- lapply(seq_along(weights), function(j)\n",
    "  unlist(weights[[j]])-(lr/nmb)*unlist(nabla_w[[j]]))\n",
    "biases <- lapply(seq_along(biases), function(j)\n",
    "  unlist(biases[[j]])-(lr/nmb)*unlist(nabla_b[[j]]))\n",
    "```\n",
    "In other words: $weights = weights - (learningrate/numberinbatch)*nablaweights$\n",
    "\n",
    "The `backprop` function applies the backpropogation algorithm to calculate the partial derivatives (to update mini-batch).\n",
    "\n",
    "The **forward step**, goes through the network layer-by-layer and calculates the output of the activation function to calculate the delta (cost gradient given the prediction). For example, the activations in layer `l` are:\n",
    "\n",
    "\\begin{equation*} \n",
    "   a^l = \\sigma(w^la^{l-1}+b^l)\n",
    "\\end{equation*}\n",
    "\n",
    "The **backward step** propogates the partial derivative (deltas) across all the neurons so that they get a share proportional to their contribution to the output.\n",
    "\n",
    "We can train our neural-network on the MNIST data-set like so:\n",
    "\n",
    "```R\n",
    "trained_net <- neuralnetwork(\n",
    "    sizes=c(in_n, 100, out_n),\n",
    "    training_data=training_data,\n",
    "    epochs=30, \n",
    "    mini_batch_size=10,\n",
    "    lr=3,\n",
    "    C='ce',\n",
    "    verbose=TRUE,\n",
    "    validation_data=testing_data)\n",
    "```\n",
    "\n",
    "At the end we get an accuracy close to 97%.\n",
    "\n",
    "For the previous two classifiers we plotted a line that showed us what the prediction would be, given a certain input. To help understand neural-networks we can do the same. Imagine, now we have a data-set that contains 2 variables (X1 and X2) and 2 classes (red and black):\n",
    "\n",
    "![Figure1](pic_support/nn_2.png)\n",
    "\n",
    "Let's train a neural-network with 50 units in the hidden-layer:\n",
    "\n",
    "```R\n",
    "sizes=c(2,50,2)\n",
    "```\n",
    "\n",
    "![Figure1](pic_support/nn_3.png)\n",
    "\n",
    "We plot our original data-set over it above. The red areas are the combination of X1 and X2 that our neural network believes correspond to the 'red' class more than the 'black' class and vice versa.\n",
    "\n",
    "We can reduce the units in our hidden-layer to see how this affects the classification:\n",
    "\n",
    "![Figure1](pic_support/nn_4.png)\n",
    "![Figure1](pic_support/nn_5.png)\n",
    "![Figure1](pic_support/nn_6.png)\n",
    "\n",
    "However, once we drop below 4 - the network doesn't have the parameters to physically fit around the spiral:\n",
    "\n",
    "![Figure1](pic_support/nn_7.png)\n",
    "![Figure1](pic_support/nn_8.png)\n",
    "\n",
    "With just one unit we can only draw a linear boundary-line:\n",
    "![Figure1](pic_support/nn_9.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
