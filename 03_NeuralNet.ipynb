{"nbformat_minor": 2, "cells": [{"source": "## Feed-Forward Neural Net", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 152, "cell_type": "code", "source": "# Reproduce results\nset.seed(1234567)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### 1. Neural Net in R (5 short functions to care about)", "cell_type": "markdown", "metadata": {}}, {"source": "Initialise neural network biase and weights matricies", "cell_type": "markdown", "metadata": {}}, {"execution_count": 153, "cell_type": "code", "source": "neuralnetwork <- function(sizes)\n{\n  num_layers <- length(sizes)\n  listw <- sizes[1:length(sizes)-1] # Skip last (weights from 1st to 2nd-to-last)\n  listb <-  sizes[-1]  # Skip first element (biases from 2nd to last)\n  \n  # Initialise with gaussian distribution for biases and weights\n  biases <- lapply(seq_along(listb), function(idx){\n    r <- listb[[idx]]\n    matrix(rnorm(n=r), nrow=r, ncol=1)\n  })\n  weights <- lapply(seq_along(listb), function(idx){\n    c <- listw[[idx]]\n    r <- listb[[idx]]\n    matrix(rnorm(n=r*c), nrow=r, ncol=c)\n  })\n  \n  # Return\n  list(sizes, num_layers, biases, weights)\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Return the derivative of the cost function (quadratic or cross-entropy)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 154, "cell_type": "code", "source": "cost_delta <- function(method, z, a, y)\n{\n  if (method=='mse'){\n    # COST: 0.5*linalg.norm(a-y)**2\n    return (a-y)*sigmoid_prime(z)\n  } else if(method=='ce'){\n    # COST: sum(-y*log(a)-(1-y)*log(1-a))\n    return (a-y)\n  }\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Perform stochastic-gradient descent to minimise cost function", "cell_type": "markdown", "metadata": {}}, {"execution_count": 155, "cell_type": "code", "source": "SGD <- function(training_data, testing_data, epochs, mini_batch_size, lr, C, sizes, num_layers, biases, weights, verbose=FALSE)\n{\n  # Every epoch\n  for (j in 1:epochs){\n    # Stochastic mini-batch (shuffle data)\n    training_data <- sample(training_data)\n    # Partition set into mini-batches\n    mini_batches <- split(training_data, \n                          ceiling(seq_along(training_data)/mini_batch_size))\n    # Feed forward (and back) all mini-batches\n    for (k in 1:length(mini_batches)) {\n      # Update biases and weights\n      res <- update_mini_batch(mini_batches[[k]], lr, C, sizes, num_layers, biases, weights)\n      biases <- res[[1]]\n      weights <- res[[-1]]\n    }\n    # Logging\n    if(verbose){if(j %% 1 == 0){\n      cat(\"Epoch: \", j, \" complete\")\n      # Print acc and hide confusion matrix\n      confusion <- evaluate(testing_data, biases, weights)\n      }}\n  }\n  cat(\"Training complete\")\n  # Return trained biases and weights\n  list(biases, weights)\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Update the biase and weights matricies for each mini-batch", "cell_type": "markdown", "metadata": {}}, {"execution_count": 156, "cell_type": "code", "source": "update_mini_batch <- function(mini_batch, lr, C, sizes, num_layers, biases, weights)\n{\n  nmb <- length(mini_batch)\n  listw <- sizes[1:length(sizes)-1] \n  listb <-  sizes[-1]  \n  \n  # Initialise updates with zero vectors (for EACH mini-batch)\n  nabla_b <- lapply(seq_along(listb), function(idx){\n    r <- listb[[idx]]\n    matrix(0, nrow=r, ncol=1)\n  })\n  nabla_w <- lapply(seq_along(listb), function(idx){\n    c <- listw[[idx]]\n    r <- listb[[idx]]\n    matrix(0, nrow=r, ncol=c)\n  })  \n  \n  # Go through mini_batch\n  for (i in 1:nmb){\n    x <- mini_batch[[i]][[1]]\n    y <- mini_batch[[i]][[-1]]\n    # Back propogation will return delta\n    # Backprop for each obeservation in mini-batch\n    delta_nablas <- backprop(x, y, C, sizes, num_layers, biases, weights)\n    delta_nabla_b <- delta_nablas[[1]]\n    delta_nabla_w <- delta_nablas[[-1]]\n    # Add on deltas to nabla\n    nabla_b <- lapply(seq_along(biases),function(j)\n      unlist(nabla_b[[j]])+unlist(delta_nabla_b[[j]]))\n    nabla_w <- lapply(seq_along(weights),function(j)\n      unlist(nabla_w[[j]])+unlist(delta_nabla_w[[j]]))\n  }\n  # After mini-batch has finished update biases and weights:\n  # i.e. weights = weights - (learning-rate/numbr in batch)*nabla_weights\n  # Opposite direction of gradient\n  weights <- lapply(seq_along(weights), function(j)\n    unlist(weights[[j]])-(lr/nmb)*unlist(nabla_w[[j]]))\n  biases <- lapply(seq_along(biases), function(j)\n    unlist(biases[[j]])-(lr/nmb)*unlist(nabla_b[[j]]))\n  # Return\n  list(biases, weights)\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Backpropogation algorithm to calculate partial derivatives using chain-rule (to update mini-batch)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 157, "cell_type": "code", "source": "backprop <- function(x, y, C, sizes, num_layers, biases, weights)\n{\n  # Initialise updates with zero vectors\n  listw <- sizes[1:length(sizes)-1] \n  listb <-  sizes[-1]  \n  \n  # Initialise updates with zero vectors (for EACH mini-batch)\n  nabla_b_backprop <- lapply(seq_along(listb), function(idx){\n    r <- listb[[idx]]\n    matrix(0, nrow=r, ncol=1)\n  })\n  nabla_w_backprop <- lapply(seq_along(listb), function(idx){\n    c <- listw[[idx]]\n    r <- listb[[idx]]\n    matrix(0, nrow=r, ncol=c)\n  })  \n  \n  # First:\n  # Feed-forward (get predictions)\n  activation <- matrix(x, nrow=length(x), ncol=1)\n  activations <- list(matrix(x, nrow=length(x), ncol=1))\n  # z = f(w.x + b)\n  # So need zs to store all z-vectors\n  zs <- list()\n  for (f in 1:length(biases)){\n    b <- biases[[f]]\n    w <- weights[[f]]\n    w_a <- w%*%activation\n    b_broadcast <- matrix(b, nrow=dim(w_a)[1], ncol=dim(w_a)[-1])\n    z <- w_a + b\n    zs[[f]] <- z\n    activation <- sigmoid(z)\n    activations[[f+1]] <- activation  # Activations already contain one element\n  }\n  # Second:\n  # Backwards (update gradient using errors)\n  # Last layer\n  delta <- cost_delta(method=C, z=zs[[length(zs)]], a=activations[[length(activations)]], y=y)\n  nabla_b_backprop[[length(nabla_b_backprop)]] <- delta\n  nabla_w_backprop[[length(nabla_w_backprop)]] <- delta %*% t(activations[[length(activations)-1]])\n  # Second to second-to-last-layer\n  for (k in 2:(num_layers-1)) {\n    sp <- sigmoid_prime(zs[[length(zs)-(k-1)]])\n    delta <- (t(weights[[length(weights)-(k-2)]]) %*% delta) * sp\n    nabla_b_backprop[[length(nabla_b_backprop)-(k-1)]] <- delta\n    testyy <- t(activations[[length(activations)-k]])\n    nabla_w_backprop[[length(nabla_w_backprop)-(k-1)]] <- delta %*% testyy\n  }\n  return_nabla <- list(nabla_b_backprop, nabla_w_backprop)\n  return_nabla\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Some helper functions (ignore these)", "cell_type": "markdown", "metadata": {}}, {"source": "#### These run a prediction on test-data and evaluate", "cell_type": "markdown", "metadata": {}}, {"execution_count": 158, "cell_type": "code", "source": "feedforward <- function(a, biases, weights)\n{\n  for (f in 1:length(biases)){\n    a <- matrix(a, nrow=length(a), ncol=1)\n    b <- biases[[f]]\n    w <- weights[[f]]\n    # (py) a = sigmoid(np.dot(w, a) + b)\n    # Equivalent of python np.dot(w,a)\n    w_a <- w%*%a\n    # Need to manually broadcast b to conform to np.dot(w,a)\n    b_broadcast <- matrix(b, nrow=dim(w_a)[1], ncol=dim(w_a)[-1])\n    a <- sigmoid(w_a + b_broadcast)\n  }\n  a\n}\n\nevaluate <- function(testing_data, biases, weights)\n{\n  predictions <- list()\n  truths <- list()\n  # Probably can avoid the for-loop here but this function run only once\n  for (i in 1:length(testing_data)){\n    test_data_chunk <- testing_data[[i]]\n    test_x <- test_data_chunk[[1]]\n    test_y <- test_data_chunk[[-1]]\n    predictions[i] <- which.max(feedforward(test_x, biases, weights))\n    truths[i] <- which.max(test_y)\n  }\n  correct <- sum(mapply(function(x,y) x==y, predictions, truths))\n  total <- length(testing_data)\n  # Print accuracy\n  print(correct/total)\n  # Return confusion\n  res <- as.data.frame(cbind(t(as.data.frame(predictions)), t(as.data.frame(truths))))\n  colnames(res) <- c(\"Prediction\", \"Truth\")\n  table(as.vector(res$Prediction), as.vector(res$Truth))\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### These are maths helpers", "cell_type": "markdown", "metadata": {}}, {"execution_count": 159, "cell_type": "code", "source": "# Calculate activation function\nsigmoid <- function(z){1.0/(1.0+exp(-z))}\n\n# Partial derivative of activation function\nsigmoid_prime <- function(z){sigmoid(z)*(1-sigmoid(z))}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### This loads data into a format net accepts", "cell_type": "markdown", "metadata": {}}, {"execution_count": 160, "cell_type": "code", "source": "train_test_from_df <- function(df, predict_col_index, train_ratio, scale_input=TRUE)\n{\n  # Helper functions\n  # Function to encode factor column as N-dummies\n  dmy <- function(df)\n  {\n    # Select only factor columns\n    factor_columns <- which(sapply(df, is.factor))\n    if (length(factor_columns) > 0)\n    {\n      # Split factors into dummies\n      dmy_enc <- model.matrix(~. + 0, data=df[factor_columns], \n                              contrasts.arg = lapply(df[factor_columns], contrasts, contrasts=FALSE))\n      dmy_enc <- as.data.frame(dmy_enc)\n      # Attach factors to df\n      df <- cbind(df, dmy_enc)\n      # Delete original columns\n      df[c(factor_columns)] <- NULL\n    }\n    df\n  }\n  \n  # Function to standarise inputs to range(0, 1)\n  scalemax <- function(df)\n  {\n    numeric_columns <- which(sapply(df, is.numeric))\n    if (length(numeric_columns)){df[numeric_columns] <- lapply(df[numeric_columns], function(x){\n      denom <- ifelse(max(x)==0, 1, max(x))\n      x/denom\n    })}\n    df\n  }\n\n  # Function to convert df to list of rows\n  listfromdf <- function(df){as.list(as.data.frame(t(df)))}\n  \n  \n  # Omit NAs (allow other options later)\n  df <- na.omit(df)\n  # Get list for X-data\n  if (scale_input){\n    X_data <- listfromdf(dmy(scalemax(df[-c(predict_col_index)])))\n  } else {\n    X_data <- listfromdf(dmy(df[-c(predict_col_index)]))\n  }\n  # Get list for y-data\n  y_data <- listfromdf(dmy(df[c(predict_col_index)]))\n  # Combine X,y\n  all_data <- list()\n  for (i in 1:length(X_data)){\n    all_data[[i]] <- c(X_data[i], y_data[i])\n  }\n  # Shuffle before splitting\n  all_data <- sample(all_data)\n  # Split to training and test\n  tr_n <- round(length(all_data)*train_ratio)\n  # Return (training, testing)\n  list(all_data[c(1:tr_n)], all_data[-c(1:tr_n)])\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Run some examples", "cell_type": "markdown", "metadata": {}}, {"source": "### 1. Iris", "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": "head(iris)", "outputs": [{"output_type": "display_data", "data": {"text/plain": "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1 5.1          3.5         1.4          0.2         setosa \n2 4.9          3.0         1.4          0.2         setosa \n3 4.7          3.2         1.3          0.2         setosa \n4 4.6          3.1         1.5          0.2         setosa \n5 5.0          3.6         1.4          0.2         setosa \n6 5.4          3.9         1.7          0.4         setosa ", "text/markdown": "\nSepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | \n|---|---|---|---|---|---|\n| 5.1    | 3.5    | 1.4    | 0.2    | setosa | \n| 4.9    | 3.0    | 1.4    | 0.2    | setosa | \n| 4.7    | 3.2    | 1.3    | 0.2    | setosa | \n| 4.6    | 3.1    | 1.5    | 0.2    | setosa | \n| 5.0    | 3.6    | 1.4    | 0.2    | setosa | \n| 5.4    | 3.9    | 1.7    | 0.4    | setosa | \n\n\n", "text/html": "<table>\n<thead><tr><th scope=col>Sepal.Length<\/th><th scope=col>Sepal.Width<\/th><th scope=col>Petal.Length<\/th><th scope=col>Petal.Width<\/th><th scope=col>Species<\/th><\/tr><\/thead>\n<tbody>\n\t<tr><td>5.1   <\/td><td>3.5   <\/td><td>1.4   <\/td><td>0.2   <\/td><td>setosa<\/td><\/tr>\n\t<tr><td>4.9   <\/td><td>3.0   <\/td><td>1.4   <\/td><td>0.2   <\/td><td>setosa<\/td><\/tr>\n\t<tr><td>4.7   <\/td><td>3.2   <\/td><td>1.3   <\/td><td>0.2   <\/td><td>setosa<\/td><\/tr>\n\t<tr><td>4.6   <\/td><td>3.1   <\/td><td>1.5   <\/td><td>0.2   <\/td><td>setosa<\/td><\/tr>\n\t<tr><td>5.0   <\/td><td>3.6   <\/td><td>1.4   <\/td><td>0.2   <\/td><td>setosa<\/td><\/tr>\n\t<tr><td>5.4   <\/td><td>3.9   <\/td><td>1.7   <\/td><td>0.4   <\/td><td>setosa<\/td><\/tr>\n<\/tbody>\n<\/table>\n", "text/latex": "\\begin{tabular}{r|lllll}\n Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n\\hline\n\t 5.1    & 3.5    & 1.4    & 0.2    & setosa\\\\\n\t 4.9    & 3.0    & 1.4    & 0.2    & setosa\\\\\n\t 4.7    & 3.2    & 1.3    & 0.2    & setosa\\\\\n\t 4.6    & 3.1    & 1.5    & 0.2    & setosa\\\\\n\t 5.0    & 3.6    & 1.4    & 0.2    & setosa\\\\\n\t 5.4    & 3.9    & 1.7    & 0.4    & setosa\\\\\n\\end{tabular}\n"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "train_test_split <- train_test_from_df(df = iris, predict_col_index = 5, train_ratio = 0.7, scale_input = TRUE)\ntraining_data <- train_test_split[[1]]\ntesting_data <- train_test_split[[2]]\n\ninput_neurons <- length(training_data[[1]][[1]])\noutput_neurons <- length(training_data[[1]][[-1]])\n\n# [4, 40, 3] \ncreate_neural_net <- neuralnetwork(c(input_neurons, \n                                     40,\n                                     output_neurons))\n\ntrained_net <- SGD(training_data=training_data,\n                   testing_data=testing_data,\n                   epochs=30, \n                   mini_batch_size=10,\n                   lr=0.5,\n                   C='ce',\n                   sizes=create_neural_net[[1]],\n                   num_layers=create_neural_net[[2]],\n                   biases=create_neural_net[[3]], \n                   weights=create_neural_net[[4]],\n                   verbose=TRUE)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Epoch:  1  complete[1] 0.2888889\nEpoch:  2  complete[1] 0.2888889\nEpoch:  3  complete[1] 0.7111111\nEpoch:  4  complete[1] 0.9555556\nEpoch:  5  complete[1] 0.6888889\nEpoch:  6  complete[1] 0.8666667\nEpoch:  7  complete[1] 0.7111111\nEpoch:  8  complete[1] 0.7555556\nEpoch:  9  complete[1] 0.8444444\nEpoch:  10  complete[1] 0.7111111\nEpoch:  11  complete[1] 0.6888889\nEpoch:  12  complete[1] 0.7111111\nEpoch:  13  complete[1] 0.7111111\nEpoch:  14  complete[1] 0.8444444\nEpoch:  15  complete[1] 0.7111111\nEpoch:  16  complete[1] 0.7111111\nEpoch:  17  complete[1] 0.9333333\nEpoch:  18  complete[1] 0.8444444\nEpoch:  19  complete[1] 0.7111111\nEpoch:  20  complete[1] 0.9555556\nEpoch:  21  complete[1] 0.8444444\nEpoch:  22  complete[1] 0.7333333\nEpoch:  23  complete[1] 0.7333333\nEpoch:  24  complete[1] 0.8444444\nEpoch:  25  complete[1] 0.7333333\nEpoch:  26  complete[1] 0.9333333\nEpoch:  27  complete[1] 0.8444444\nEpoch:  28  complete[1] 0.8444444\nEpoch:  29  complete[1] 0.9555556\nEpoch:  30  complete[1] 0.9777778\nTraining complete"}], "metadata": {"collapsed": false}}, {"execution_count": 40, "cell_type": "code", "source": "# Trained matricies:\nbiases <- trained_net[[1]]\nweights <- trained_net[[-1]]\n\n# Accuracy (train)\nevaluate(training_data, biases, weights)  #0.94\n# Accuracy (test)\nevaluate(testing_data, biases, weights)  #0.97", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1] 0.9428571\n"}, {"output_type": "display_data", "data": {"text/plain": "   \n     1  2  3\n  1 32  0  0\n  2  0 37  6\n  3  0  0 30"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "[1] 0.9777778\n"}, {"output_type": "display_data", "data": {"text/plain": "   \n     1  2  3\n  1 18  0  0\n  2  0 13  1\n  3  0  0 13"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "### 2. MNIST", "cell_type": "markdown", "metadata": {}}, {"execution_count": 47, "cell_type": "code", "source": "# Here we have splits for train-test already (may take a minute to download)\n# Train\nmnist <- read.table('https://iliadl.blob.core.windows.net/nnet/mnist_train.csv', sep=\",\", header = FALSE)\nmnist$V1 <- factor(mnist$V1)\ntraining_data <- train_test_from_df(df = mnist, predict_col_index = 1, train_ratio = 1, scale_input = TRUE)[[1]]\n\n# Test\nmnist <- read.table('https://iliadl.blob.core.windows.net/nnet/mnist_test.csv', sep=\",\", header = FALSE)\nmnist$V1 <- factor(mnist$V1)\ntesting_data <- train_test_from_df(df = mnist, predict_col_index = 1, train_ratio = 1, scale_input = TRUE)[[1]]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "What does the data exactly look like?", "cell_type": "markdown", "metadata": {}}, {"execution_count": 161, "cell_type": "code", "source": "example_entry <- training_data[[1]]\nexample_x <- example_entry[[1]]\nexample_y <- example_entry[[2]]\n\n# Y-vector looks like this:\nprint(example_y)\n# It corresponds to digit:\nprint(which.max(example_y)-1)\n\n# X-vector has length\nprint(length(example_x))\n\n# We can think of it as a 28x28 matrix where entries are a shade of gray\nex_plot <- matrix(example_x, nrow=28, ncol=28, byrow=FALSE)\nimage(ex_plot, axes = FALSE, col = grey(seq(0, 1, length = 256)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": " [1] 0 0 0 1 0 0 0 0 0 0\n[1] 3\n[1] 784\n"}, {"output_type": "display_data", "data": {"image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAA8FBMVEUAAAADAwMEBAQFBQUL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\nBggJBggJBggJBggJBggJBggJBggJBvwPLGUyUKBmYbYAAAAASUVORK5CYII=", "text/plain": "plot without title"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "Let's train a neural net with one 100-neuron hidden-layer to predict (given 784 vector of gray intensity) the digit (from 0 to 9)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 164, "cell_type": "code", "source": "# Input and output neurons\ninput_neurons <- length(training_data[[1]][[1]])\noutput_neurons <- length(training_data[[1]][[-1]])\n\n# Step 1. Initialise nueral network (bias and weights for layers)\n# MNIST: 784, 100, 10 (one hidden-layer)\ncreate_neural_net <- neuralnetwork(c(input_neurons, 100, output_neurons))\n\n# Step 2. Train NN using SGD\nprint(\"THIS WILL TAKE 20-30 MINUTES...\")\ntrained_net <- SGD(training_data=training_data,\n                   testing_data=testing_data,\n                   epochs=30, \n                   mini_batch_size=10,\n                   lr=3,\n                   C='ce',\n                   sizes=create_neural_net[[1]],\n                   num_layers=create_neural_net[[2]],\n                   biases=create_neural_net[[3]], \n                   weights=create_neural_net[[4]],\n                   verbose=TRUE)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Epoch:  1  completeNULL\n[1] 0.8966\nEpoch:  2  completeNULL\n[1] 0.9319\nEpoch:  3  completeNULL\n[1] 0.9416\nEpoch:  4  completeNULL\n[1] 0.9376\nEpoch:  5  completeNULL\n[1] 0.9478\nEpoch:  6  completeNULL\n[1] 0.95\nEpoch:  7  completeNULL\n[1] 0.9448\nEpoch:  8  completeNULL\n[1] 0.9464\nEpoch:  9  completeNULL\n[1] 0.9535\nEpoch:  10  completeNULL\n[1] 0.9544\nEpoch:  11  completeNULL\n[1] 0.956\nEpoch:  12  completeNULL\n[1] 0.9577\nEpoch:  13  completeNULL\n[1] 0.9571\nEpoch:  14  completeNULL\n[1] 0.9582\nEpoch:  15  completeNULL\n[1] 0.96\nEpoch:  16  completeNULL\n[1] 0.9603\nEpoch:  17  completeNULL\n[1] 0.9605\nEpoch:  18  completeNULL\n[1] 0.9617\nEpoch:  19  completeNULL\n[1] 0.961\nEpoch:  20  completeNULL\n[1] 0.959\nEpoch:  21  completeNULL\n[1] 0.9623\nEpoch:  22  completeNULL\n[1] 0.9628\nEpoch:  23  completeNULL\n[1] 0.9636\nEpoch:  24  completeNULL\n[1] 0.9607\nEpoch:  25  completeNULL\n[1] 0.9614\nEpoch:  26  completeNULL\n[1] 0.9647\nEpoch:  27  completeNULL\n[1] 0.9622\nEpoch:  28  completeNULL\n[1] 0.9639\nEpoch:  29  completeNULL\n[1] 0.9643\nEpoch:  30  completeNULL\n[1] 0.9652\nTraining complete"}], "metadata": {"collapsed": false}}, {"execution_count": 165, "cell_type": "code", "source": "# Trained matricies:\nbiases <- trained_net[[1]]\nweights <- trained_net[[-1]]\n\n# Accuracy (train)\n# CONFUSION TRAIN MATRIX\nevaluate(training_data, biases, weights)  #0.98\n# Accuracy (test)\n# CONFUSION TEST MATRIX\nevaluate(testing_data, biases, weights)  #0.96", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[1] 0.9821\n"}, {"output_type": "display_data", "data": {"text/plain": "    \n        1    2    3    4    5    6    7    8    9   10\n  1  5877    1   10   12   11   10   25    2   12   21\n  2     1 6683    3    3    6    1    3    7    8    4\n  3    12   13 5899   53   15    4    9   35   37    9\n  4     2    5   12 5977    3   34    1   11   32   49\n  5     3    7    7    0 5756    2    8   10   13   83\n  6     1    1    2   26    1 5325   16    1   28   27\n  7     5    1    5    0   12   23 5846    0    9    4\n  8     0    8    7    9    5    2    0 6179    3   34\n  9    19   20   13   36    3   12   10    8 5702   36\n  10    3    3    0   15   30    8    0   12    7 5682"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "[1] 0.9652\n"}, {"output_type": "display_data", "data": {"text/plain": "    \n        1    2    3    4    5    6    7    8    9   10\n  1   969    0    5    2    1    8   14    1   10    3\n  2     0 1120    2    0    1    0    3    4    2    3\n  3     3    5 1001    9    3    0    3   23   10    3\n  4     2    3    8  987    0   14    0    3   10   12\n  5     0    0    3    0  951    1    6    4    6   23\n  6     0    1    0    6    0  856    5    0    4    9\n  7     2    0    2    0    4    4  922    0    5    0\n  8     1    0    7    2    4    1    1  985    3    9\n  9     3    6    4    4    2    4    4    1  921    7\n  10    0    0    0    0   16    4    0    7    3  940"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 169, "cell_type": "code", "source": "# Test this out with one example\n# Do some machine-learning\ntest_entry <- testing_data[[42]]\ntest_x <- test_entry[[1]]\ntest_y <- test_entry[[2]]\n\n# Input\nimage(matrix(test_x, nrow=28, ncol=28, byrow=FALSE),\n      axes = FALSE, col = grey(seq(0, 1, length = 256)))\n\n# Output\nwhich.max(feedforward(test_x, biases, weights))-1", "outputs": [{"output_type": "display_data", "data": {"text/plain": "[1] 8", "text/markdown": "8", "text/html": "8", "text/latex": "8"}, "metadata": {}}, {"output_type": "display_data", "data": {"image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAABLFBMVEUAAAABAQECAgIDAwMF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\nNBASNBASNBASNPg/sdUu3YkTZhEAAAAASUVORK5CYII=", "text/plain": "plot without title"}, "metadata": {}}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "R", "name": "r", "language": "R"}, "language_info": {"mimetype": "text/x-r-source", "version": "3.3.0", "name": "R", "pygments_lexer": "r", "file_extension": ".r", "codemirror_mode": "r"}}}